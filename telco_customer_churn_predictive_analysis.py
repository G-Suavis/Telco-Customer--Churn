# -*- coding: utf-8 -*-
"""Telco-Customer-Churn predictive analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QrRCA_AIDgOcgvHgZSR_xn8TvJ8gfiTF
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load dataset
file_path = "Telco_customer_churn.xlsx" # Replace with your file path
df = pd.read_excel(file_path)

# Preprocessing
# Drop irrelevant columns
columns_to_drop = ['CustomerID', 'Country', 'State', 'City', 'Lat Long', 'Churn Reason', 'Churn Score']
df = df.drop(columns=columns_to_drop, errors='ignore')

# Convert 'Churn Label' to numeric (0 for 'No', 1 for 'Yes')
df['Churn Label'] = df['Churn Label'].apply(lambda x: 1 if x == 'Yes' else 0)

# Convert Total Charges to numeric
df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce')
df['Total Charges'] = df['Total Charges'].fillna(0)

# Encode categorical variables
binary_columns = ['Gender', 'Senior Citizen', 'Partner', 'Dependents', 'Phone Service', 'Paperless Billing']
label_encoders = {}
for col in binary_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# One-hot encode multi-class columns
multi_class_columns = ['Multiple Lines', 'Internet Service', 'Online Security', 'Online Backup',
                       'Device Protection', 'Tech Support', 'Streaming TV', 'Streaming Movies',
                       'Contract', 'Payment Method']
df = pd.get_dummies(df, columns=multi_class_columns, drop_first=True)

# Define features and target
X = df.drop(columns=['Churn Label', 'Churn Value'], errors='ignore')  # Drop both Churn Label and Churn Value
y = df['Churn Label']  # Use Churn Label as the target variable

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Scale features for Logistic Regression
scaler = StandardScaler()
X_resampled_scaled = scaler.fit_transform(X_resampled)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_resampled_scaled, y_resampled, test_size=0.3, random_state=42)

# Define models to compare
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "LightGBM": LGBMClassifier(random_state=42),
    "SVM (Linear Kernel)": SVC(kernel='linear', probability=True, random_state=42)
}

# Compare models
results = []
for model_name, model in models.items():
    # Train model
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    # Evaluate performance
    accuracy = accuracy_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else "N/A"
    report = classification_report(y_test, y_pred, output_dict=True)

    # Handle missing classes in report
    precision_churn = report['1']['precision'] if '1' in report else 0.0
    recall_churn = report['1']['recall'] if '1' in report else 0.0
    f1_score_churn = report['1']['f1-score'] if '1' in report else 0.0

    # Store results
    results.append({
        "Model": model_name,
        "Accuracy": accuracy,
        "AUC-ROC": auc_roc,
        "Precision (Churn)": precision_churn,
        "Recall (Churn)": recall_churn,
        "F1-Score (Churn)": f1_score_churn
    })

# Display results
results_df = pd.DataFrame(results).sort_values(by="AUC-ROC", ascending=False)
print(results_df)

# Save results to an Excel file
results_df.to_excel("model_comparison_results.xlsx", index=False)

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.feature_selection import RFE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import shap

# Load dataset
file_path = "Telco_customer_churn.xlsx"  # Replace with your file path
df = pd.read_excel(file_path)

# Preprocessing
# Drop irrelevant columns
columns_to_drop = ['CustomerID', 'Country', 'State', 'City', 'Lat Long', 'Churn Reason', 'Churn Score']
df = df.drop(columns=columns_to_drop, errors='ignore')

# Convert 'Churn Label' to numeric
df['Churn Label'] = df['Churn Label'].apply(lambda x: 1 if x == 'Yes' else 0)

# Convert Total Charges to numeric
df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce')
df['Total Charges'] = df['Total Charges'].fillna(0)

# Encode categorical variables
binary_columns = ['Gender', 'Senior Citizen', 'Partner', 'Dependents', 'Phone Service', 'Paperless Billing']
label_encoders = {}
for col in binary_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

multi_class_columns = ['Multiple Lines', 'Internet Service', 'Online Security', 'Online Backup',
                       'Device Protection', 'Tech Support', 'Streaming TV', 'Streaming Movies',
                       'Contract', 'Payment Method']
df = pd.get_dummies(df, columns=multi_class_columns, drop_first=True)

# Define features and target
X = df.drop(columns=['Churn Label', 'Churn Value'], errors='ignore')
y = df['Churn Label']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Model Feature Importance Extraction
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
}

feature_importance = pd.DataFrame({'Feature': X.columns})

for model_name, model in models.items():
    model.fit(X_train_scaled, y_train)
    if hasattr(model, "feature_importances_"):
        # Tree-based models
        feature_importance[model_name] = model.feature_importances_
    elif hasattr(model, "coef_"):
        # Logistic Regression
        feature_importance[model_name] = np.abs(model.coef_[0])

# Average Importance
feature_importance['AverageImportance'] = feature_importance.iloc[:, 1:].mean(axis=1)
feature_importance = feature_importance.sort_values(by='AverageImportance', ascending=False)

print("Top Features Based on Average Importance:")
print(feature_importance.head(10))

# Recursive Feature Elimination (RFE) with Logistic Regression
rfe = RFE(estimator=LogisticRegression(max_iter=1000, random_state=42), n_features_to_select=10)
rfe.fit(X_train_scaled, y_train)
rfe_features = X.columns[rfe.support_]
print("\nTop Features Identified by RFE:")
print(rfe_features)

# SHAP Values for XGBoost
explainer = shap.Explainer(models['XGBoost'], X_train_scaled)
shap_values = explainer(X_train_scaled)
shap.summary_plot(shap_values, X_train, plot_type="bar")

# Final Output
final_features = feature_importance.head(10)['Feature'].tolist()
print("\nFinal Selected Features for All Models:")
print(final_features)